{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Data loaded successfully\n",
      "INFO:__main__:Sampling 1000 rows from data and 500 rows from genre_data...\n",
      "INFO:__main__:Data sampling completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading data...\")\n",
    "data = pd.read_csv(\"dataset/data.csv\")\n",
    "genre_data = pd.read_csv('dataset/data_by_genres.csv')\n",
    "year_data = pd.read_csv('dataset/data_by_year.csv')\n",
    "logger.info(\"Data loaded successfully\")\n",
    "\n",
    "# Limit the size of the dataset by sampling a subset\n",
    "data_sample_size = 1000  # Adjust this number as needed\n",
    "genre_data_sample_size = 500  # Adjust this number as needed\n",
    "\n",
    "logger.info(f\"Sampling {data_sample_size} rows from data and {genre_data_sample_size} rows from genre_data...\")\n",
    "data = data.sample(n=data_sample_size, random_state=42)\n",
    "genre_data = genre_data.sample(n=genre_data_sample_size, random_state=42)\n",
    "logger.info(\"Data sampling completed\")\n",
    "\n",
    "# Create a pipeline and fit it in one go\n",
    "cluster_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('kmeans', KMeans(n_clusters=10))\n",
    "])\n",
    "genre_data['cluster'] = cluster_pipeline.fit_predict(genre_data.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Create a pipeline for song clustering\n",
    "song_cluster_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('kmeans', KMeans(n_clusters=20, verbose=False))\n",
    "])\n",
    "X = data.select_dtypes(np.number)\n",
    "song_cluster_pipeline.fit(X)\n",
    "data['cluster_label'] = song_cluster_pipeline.predict(X)\n",
    "\n",
    "# Create the models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the trained model and data\n",
    "joblib.dump(song_cluster_pipeline, 'models/song_cluster_pipeline.pkl')\n",
    "data.to_csv('dataset/data_sampled.csv', index=False)\n",
    "genre_data.to_csv('dataset/genre_data_sampled.csv', index=False)\n",
    "logger.info(\"Model and data saved successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
